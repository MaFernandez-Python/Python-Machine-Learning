{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "nbTranslate": {
      "displayLangs": [
        "*"
      ],
      "hotkey": "alt-t",
      "langInMainMenu": true,
      "sourceLang": "en",
      "targetLang": "fr",
      "useGoogleTranslate": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaFernandez-Python/Python-Machine-Learning/blob/master/151_1_1_Perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtYbBwLwU2Dy"
      },
      "source": [
        "<img src=\"https://datascientest.fr/train/assets/logo_datascientest.png\" style=\"height:150px\">\n",
        "\n",
        "\n",
        "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
        "<center><h1>Introduction au Deep Learning avec Keras</h1></center>\n",
        "<center><h2>L'algorithme du Perceptron</h2></center>\n",
        "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
        "\n",
        "\n",
        "> Dans cet exercice, nous allons introduire le principe de fonctionnement de l'algorithme du perceptron. Pour se faire, nous allons entraîner un modèle basé sur le principe du perceptron simple à partir de la base de données *Moon* de la bibliothèque *scikit-learn*.\n",
        "\n",
        "> Comme évoqué précédemment, le **choix** de la fonction d'activation dépend de **l'espace de sortie souhaité** du modèle Perceptron:\n",
        ">\n",
        "> * La fonction $\\mathbf{sigmoid}$ prend des valeurs de $(-\\infty, \\infty) $ à $ [0,1]$, ce qui en fait un choix idéal pour afficher une valeur de probabilité pour la classification. Dans ce cas, le modèle Perceptron équivaut à la **Régression logistique**.\n",
        ">\n",
        ">Dans l'exemple ci-dessous, nous allons implémenter ce modèle.\n",
        ">\n",
        "\n",
        "\n",
        "* **(a)** Exécuter la cellule suivante pour importer les packages nécéssaires :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "LilXIKXrU2Dz"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_moons\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "run_control": {
          "frozen": true
        },
        "id": "sn971gcrU2D0"
      },
      "source": [
        "* **(b)** Exécuter la cellule suivante pour stocker dans X (**features**) et y (**target**) un échantillon du dataset *Moon* à l'aide du module make_moons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "nKtd7bT8U2D0"
      },
      "source": [
        "X,y = make_moons(n_samples = 300, noise = 0.05) #300 observations choisies pour ne pas trop alourdir l'entraînement"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUN7KrWyU2D0"
      },
      "source": [
        "* **(c)** Afficher les observations en colorant chacune d'entre elles selon la classe à laquelle elle appartient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brRHIG-AU2D0"
      },
      "source": [
        "## Insérez votre code ici"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "ay67SZPZU2D1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "577c487a-b11a-418d-84d7-448d3b26af17"
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "\n",
        "X_0_x = X[y == 0][:, 0]\n",
        "X_0_y = X[y == 0][:, 1]\n",
        "\n",
        "X_1_x = X[y == 1][:, 0]\n",
        "X_1_y = X[y == 1][:, 1]\n",
        "          \n",
        "ax.scatter(X_0_x, X_0_y, color = \"#68E2BF\", label = \"Class 0\")\n",
        "ax.scatter(X_1_x, X_1_y, color = \"#163090\", label = \"Class 1\")\n",
        "\n",
        "ax.legend()\n",
        "fig.set_size_inches(8, 4.5)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x324 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaTOFdZSU2D1"
      },
      "source": [
        "* **(d)** Séparer les données en un jeu d'entraiement **80%** et de test **20%**. Utiliser l'argument **`random_state=42`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EWrjIJ5U2D1"
      },
      "source": [
        "## Insérez votre code ici"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "Q_IEiDIKU2D1"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test, y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "run_control": {
          "frozen": true
        },
        "id": "Tj9qRuDMU2D1"
      },
      "source": [
        ">Nous allons maintenant définir le 'neurone' ou perceptron par lequel passer nos données.\n",
        ">\n",
        "* Exécuter la cellule suivante afin d'importer les packages nécéssaires à l'initialisation du perceptron."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "QuxpaIpkU2D2"
      },
      "source": [
        "from tensorflow.keras.layers import Input, Dense #Pour instancier une couche Dense et une d'Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import regularizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYLwL8-AU2D2"
      },
      "source": [
        ">Dans un premier temps, nous allons définir l'**`Input`** qui prendra en compte la dimension des variables d'entrée, c'est à dire la dimension de nos variables explicatives. Les objets **Input** et **Output**, de classe *Keras Tensor*, permettent de construire des modèles à partir seulement des entrées. \n",
        ">\n",
        "* **(e)** Instancier cet **`Input`** dans inputs.\n",
        "* **(f)** Créer une couche **`Dense`** nommée **`dense1`** prenant en argument **`units=1`**, qui correspond au nombre de perceptrons de notre couche, une fonction d'activation **`sigmoid`** ainsi que l'argument **`kernel_regularizer=regularizers.l2(0.)`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TS8dghYnU2D2"
      },
      "source": [
        "## Insérez votre code ici"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "5-h7cxKuU2D2"
      },
      "source": [
        "inputs = Input(shape = 2, name = \"Input\")\n",
        "\n",
        "dense1 = Dense(units = 1, activation = \"sigmoid\",kernel_regularizer=regularizers.l2(0.), name = \"Dense_1\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zQiC-vWU2D2"
      },
      "source": [
        ">Nous allons à présent définir l'**`output`** de notre modèle. Comme ici, il n'est constitué que d'un seul perceptron, l'output correspond à notre input ) laquelle on a appliqué la couche **`dense1`**.\n",
        ">\n",
        "* **(g)** Dans **`outputs`**, appliquer la couche **`dense1`** sur L'**`Input`** précédent.\n",
        "* **(h)** Instancier un **Model** prenant en argument l'**`Input`** et l'**`output`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nifVFWlRU2D3"
      },
      "source": [
        "## Insérez votre code ici"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "dbys-zIKU2D3"
      },
      "source": [
        "outputs=dense1(inputs)\n",
        "\n",
        "model = Model(inputs = inputs, outputs = outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZpMdirLU2D3"
      },
      "source": [
        "* **(i)** Compiler le modèle précedemment créé grâce à la méthode **`.compile`**. Utiliser comme fonction de perte : **\"binary_crossentropy\"**, comme optimiseur : **\"adam\"** et pour métrique : **[\"accuracy\"]**.\n",
        "\n",
        "<div class=\"alert alert-info\">\n",
        "<i class=\"fa fa-info-circle\"></i> &emsp; \n",
        "    <b>Rappel</b> : Une fonction de perte (le terme perte a été utilisé pour la première fois par Wald, 1939) permet de mesurer  <b>un écart</b> entre les valeurs observées des données et les valeurs calculées à l'aide de la fonction d'ajustement. C'est la fonction que l'on cherche à <b>minimiser pendant l'entraînement d'un modèle</b>.     \n",
        "    <br> Dans le cadre de problèmes de classification binaire, on privilégie en général la fonction de perte binary crossentropy. On aura plus tendance à regarder l'erreur absolue ou quadratique moyenne pour des problèmes de régression.\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3T0sQfCU2D4"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDXm8okHU2D4"
      },
      "source": [
        "## Insérez votre code ici"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "3C4IG3ldU2D4"
      },
      "source": [
        "model.compile(loss = \"binary_crossentropy\",\n",
        "              optimizer = \"adam\",\n",
        "              metrics = [\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQBskc6uU2D4"
      },
      "source": [
        "* **(j)** Entrainer votre modèle sur le jeu de données d'entrainement avec comme argument : epochs=500, batch_size=32, validation_split=0.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EV2d5EV6U2D4"
      },
      "source": [
        "## Insérez votre code ici"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "Lt15vsPnU2D5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e46e8c9-312a-4698-9264-aa4fe7748e86"
      },
      "source": [
        "model.fit(X_train,y_train,epochs=500,batch_size=2, validation_split=0.2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "96/96 [==============================] - 2s 6ms/step - loss: 0.6602 - accuracy: 0.6302 - val_loss: 0.6404 - val_accuracy: 0.6042\n",
            "Epoch 2/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.6293 - accuracy: 0.6875 - val_loss: 0.6134 - val_accuracy: 0.6875\n",
            "Epoch 3/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.6006 - accuracy: 0.7240 - val_loss: 0.5896 - val_accuracy: 0.7292\n",
            "Epoch 4/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.5742 - accuracy: 0.7448 - val_loss: 0.5670 - val_accuracy: 0.7708\n",
            "Epoch 5/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.5504 - accuracy: 0.7865 - val_loss: 0.5454 - val_accuracy: 0.8125\n",
            "Epoch 6/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.5285 - accuracy: 0.8125 - val_loss: 0.5272 - val_accuracy: 0.8333\n",
            "Epoch 7/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.5091 - accuracy: 0.8125 - val_loss: 0.5098 - val_accuracy: 0.8333\n",
            "Epoch 8/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.4911 - accuracy: 0.8385 - val_loss: 0.4946 - val_accuracy: 0.8542\n",
            "Epoch 9/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.4748 - accuracy: 0.8438 - val_loss: 0.4803 - val_accuracy: 0.8542\n",
            "Epoch 10/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.4600 - accuracy: 0.8646 - val_loss: 0.4671 - val_accuracy: 0.8542\n",
            "Epoch 11/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.4466 - accuracy: 0.8698 - val_loss: 0.4549 - val_accuracy: 0.8750\n",
            "Epoch 12/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.4342 - accuracy: 0.8802 - val_loss: 0.4437 - val_accuracy: 0.8542\n",
            "Epoch 13/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.4230 - accuracy: 0.8854 - val_loss: 0.4338 - val_accuracy: 0.8542\n",
            "Epoch 14/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.4128 - accuracy: 0.8802 - val_loss: 0.4245 - val_accuracy: 0.9167\n",
            "Epoch 15/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.4034 - accuracy: 0.8750 - val_loss: 0.4160 - val_accuracy: 0.8958\n",
            "Epoch 16/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3948 - accuracy: 0.8750 - val_loss: 0.4080 - val_accuracy: 0.8958\n",
            "Epoch 17/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3870 - accuracy: 0.8750 - val_loss: 0.4006 - val_accuracy: 0.8750\n",
            "Epoch 18/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3797 - accuracy: 0.8750 - val_loss: 0.3942 - val_accuracy: 0.8750\n",
            "Epoch 19/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3732 - accuracy: 0.8698 - val_loss: 0.3876 - val_accuracy: 0.8750\n",
            "Epoch 20/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3670 - accuracy: 0.8750 - val_loss: 0.3820 - val_accuracy: 0.8750\n",
            "Epoch 21/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3613 - accuracy: 0.8750 - val_loss: 0.3766 - val_accuracy: 0.8750\n",
            "Epoch 22/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3561 - accuracy: 0.8750 - val_loss: 0.3714 - val_accuracy: 0.8750\n",
            "Epoch 23/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3513 - accuracy: 0.8698 - val_loss: 0.3668 - val_accuracy: 0.8750\n",
            "Epoch 24/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3467 - accuracy: 0.8646 - val_loss: 0.3622 - val_accuracy: 0.8750\n",
            "Epoch 25/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3425 - accuracy: 0.8646 - val_loss: 0.3582 - val_accuracy: 0.8750\n",
            "Epoch 26/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3387 - accuracy: 0.8646 - val_loss: 0.3545 - val_accuracy: 0.8750\n",
            "Epoch 27/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3350 - accuracy: 0.8646 - val_loss: 0.3507 - val_accuracy: 0.8750\n",
            "Epoch 28/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3318 - accuracy: 0.8646 - val_loss: 0.3475 - val_accuracy: 0.8750\n",
            "Epoch 29/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3285 - accuracy: 0.8594 - val_loss: 0.3440 - val_accuracy: 0.8750\n",
            "Epoch 30/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3257 - accuracy: 0.8594 - val_loss: 0.3413 - val_accuracy: 0.8750\n",
            "Epoch 31/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3229 - accuracy: 0.8594 - val_loss: 0.3385 - val_accuracy: 0.8750\n",
            "Epoch 32/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3202 - accuracy: 0.8542 - val_loss: 0.3358 - val_accuracy: 0.8750\n",
            "Epoch 33/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3178 - accuracy: 0.8542 - val_loss: 0.3333 - val_accuracy: 0.8750\n",
            "Epoch 34/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3154 - accuracy: 0.8542 - val_loss: 0.3309 - val_accuracy: 0.8750\n",
            "Epoch 35/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3134 - accuracy: 0.8542 - val_loss: 0.3286 - val_accuracy: 0.8750\n",
            "Epoch 36/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3112 - accuracy: 0.8542 - val_loss: 0.3264 - val_accuracy: 0.8750\n",
            "Epoch 37/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3094 - accuracy: 0.8542 - val_loss: 0.3245 - val_accuracy: 0.8750\n",
            "Epoch 38/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3076 - accuracy: 0.8542 - val_loss: 0.3226 - val_accuracy: 0.8750\n",
            "Epoch 39/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3059 - accuracy: 0.8542 - val_loss: 0.3206 - val_accuracy: 0.8750\n",
            "Epoch 40/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3043 - accuracy: 0.8542 - val_loss: 0.3190 - val_accuracy: 0.8750\n",
            "Epoch 41/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3027 - accuracy: 0.8542 - val_loss: 0.3174 - val_accuracy: 0.8750\n",
            "Epoch 42/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3013 - accuracy: 0.8542 - val_loss: 0.3158 - val_accuracy: 0.8750\n",
            "Epoch 43/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.3000 - accuracy: 0.8542 - val_loss: 0.3143 - val_accuracy: 0.8750\n",
            "Epoch 44/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2987 - accuracy: 0.8542 - val_loss: 0.3130 - val_accuracy: 0.8750\n",
            "Epoch 45/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2975 - accuracy: 0.8542 - val_loss: 0.3114 - val_accuracy: 0.8750\n",
            "Epoch 46/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2963 - accuracy: 0.8542 - val_loss: 0.3103 - val_accuracy: 0.8750\n",
            "Epoch 47/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2952 - accuracy: 0.8594 - val_loss: 0.3091 - val_accuracy: 0.8750\n",
            "Epoch 48/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2944 - accuracy: 0.8542 - val_loss: 0.3079 - val_accuracy: 0.8750\n",
            "Epoch 49/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2932 - accuracy: 0.8594 - val_loss: 0.3068 - val_accuracy: 0.8750\n",
            "Epoch 50/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2923 - accuracy: 0.8594 - val_loss: 0.3055 - val_accuracy: 0.8750\n",
            "Epoch 51/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2913 - accuracy: 0.8594 - val_loss: 0.3047 - val_accuracy: 0.8750\n",
            "Epoch 52/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2906 - accuracy: 0.8594 - val_loss: 0.3037 - val_accuracy: 0.8750\n",
            "Epoch 53/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2897 - accuracy: 0.8594 - val_loss: 0.3028 - val_accuracy: 0.8750\n",
            "Epoch 54/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2889 - accuracy: 0.8594 - val_loss: 0.3018 - val_accuracy: 0.8750\n",
            "Epoch 55/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2881 - accuracy: 0.8594 - val_loss: 0.3008 - val_accuracy: 0.8750\n",
            "Epoch 56/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2874 - accuracy: 0.8594 - val_loss: 0.3001 - val_accuracy: 0.8750\n",
            "Epoch 57/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2868 - accuracy: 0.8646 - val_loss: 0.2993 - val_accuracy: 0.8750\n",
            "Epoch 58/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2862 - accuracy: 0.8594 - val_loss: 0.2986 - val_accuracy: 0.8750\n",
            "Epoch 59/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2855 - accuracy: 0.8646 - val_loss: 0.2977 - val_accuracy: 0.8750\n",
            "Epoch 60/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2848 - accuracy: 0.8646 - val_loss: 0.2972 - val_accuracy: 0.8750\n",
            "Epoch 61/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2843 - accuracy: 0.8646 - val_loss: 0.2963 - val_accuracy: 0.8750\n",
            "Epoch 62/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2837 - accuracy: 0.8646 - val_loss: 0.2956 - val_accuracy: 0.8750\n",
            "Epoch 63/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2831 - accuracy: 0.8646 - val_loss: 0.2951 - val_accuracy: 0.8750\n",
            "Epoch 64/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2827 - accuracy: 0.8646 - val_loss: 0.2946 - val_accuracy: 0.8750\n",
            "Epoch 65/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2821 - accuracy: 0.8646 - val_loss: 0.2939 - val_accuracy: 0.8750\n",
            "Epoch 66/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2817 - accuracy: 0.8646 - val_loss: 0.2935 - val_accuracy: 0.8750\n",
            "Epoch 67/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2813 - accuracy: 0.8646 - val_loss: 0.2928 - val_accuracy: 0.8750\n",
            "Epoch 68/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2807 - accuracy: 0.8646 - val_loss: 0.2923 - val_accuracy: 0.8750\n",
            "Epoch 69/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2803 - accuracy: 0.8646 - val_loss: 0.2919 - val_accuracy: 0.8750\n",
            "Epoch 70/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2798 - accuracy: 0.8646 - val_loss: 0.2913 - val_accuracy: 0.8750\n",
            "Epoch 71/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2795 - accuracy: 0.8646 - val_loss: 0.2908 - val_accuracy: 0.8750\n",
            "Epoch 72/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2791 - accuracy: 0.8646 - val_loss: 0.2905 - val_accuracy: 0.8750\n",
            "Epoch 73/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2787 - accuracy: 0.8646 - val_loss: 0.2899 - val_accuracy: 0.8750\n",
            "Epoch 74/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2785 - accuracy: 0.8646 - val_loss: 0.2893 - val_accuracy: 0.8750\n",
            "Epoch 75/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2781 - accuracy: 0.8646 - val_loss: 0.2892 - val_accuracy: 0.8750\n",
            "Epoch 76/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2779 - accuracy: 0.8646 - val_loss: 0.2888 - val_accuracy: 0.8750\n",
            "Epoch 77/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2773 - accuracy: 0.8646 - val_loss: 0.2884 - val_accuracy: 0.8750\n",
            "Epoch 78/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2771 - accuracy: 0.8646 - val_loss: 0.2881 - val_accuracy: 0.8750\n",
            "Epoch 79/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2768 - accuracy: 0.8646 - val_loss: 0.2875 - val_accuracy: 0.8750\n",
            "Epoch 80/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2765 - accuracy: 0.8646 - val_loss: 0.2873 - val_accuracy: 0.8750\n",
            "Epoch 81/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2761 - accuracy: 0.8646 - val_loss: 0.2869 - val_accuracy: 0.8750\n",
            "Epoch 82/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2759 - accuracy: 0.8646 - val_loss: 0.2866 - val_accuracy: 0.8750\n",
            "Epoch 83/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2757 - accuracy: 0.8646 - val_loss: 0.2864 - val_accuracy: 0.8750\n",
            "Epoch 84/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2754 - accuracy: 0.8646 - val_loss: 0.2861 - val_accuracy: 0.8750\n",
            "Epoch 85/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2750 - accuracy: 0.8646 - val_loss: 0.2857 - val_accuracy: 0.8750\n",
            "Epoch 86/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2749 - accuracy: 0.8646 - val_loss: 0.2854 - val_accuracy: 0.8750\n",
            "Epoch 87/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2746 - accuracy: 0.8646 - val_loss: 0.2853 - val_accuracy: 0.8750\n",
            "Epoch 88/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2744 - accuracy: 0.8646 - val_loss: 0.2849 - val_accuracy: 0.8750\n",
            "Epoch 89/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2741 - accuracy: 0.8698 - val_loss: 0.2847 - val_accuracy: 0.8750\n",
            "Epoch 90/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2740 - accuracy: 0.8698 - val_loss: 0.2845 - val_accuracy: 0.8750\n",
            "Epoch 91/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2737 - accuracy: 0.8698 - val_loss: 0.2842 - val_accuracy: 0.8750\n",
            "Epoch 92/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2735 - accuracy: 0.8698 - val_loss: 0.2840 - val_accuracy: 0.8750\n",
            "Epoch 93/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2732 - accuracy: 0.8698 - val_loss: 0.2837 - val_accuracy: 0.8750\n",
            "Epoch 94/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2731 - accuracy: 0.8698 - val_loss: 0.2836 - val_accuracy: 0.8750\n",
            "Epoch 95/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2730 - accuracy: 0.8698 - val_loss: 0.2835 - val_accuracy: 0.8750\n",
            "Epoch 96/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2727 - accuracy: 0.8698 - val_loss: 0.2833 - val_accuracy: 0.8750\n",
            "Epoch 97/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2726 - accuracy: 0.8698 - val_loss: 0.2831 - val_accuracy: 0.8750\n",
            "Epoch 98/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2724 - accuracy: 0.8698 - val_loss: 0.2829 - val_accuracy: 0.8750\n",
            "Epoch 99/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2722 - accuracy: 0.8698 - val_loss: 0.2827 - val_accuracy: 0.8750\n",
            "Epoch 100/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2720 - accuracy: 0.8698 - val_loss: 0.2825 - val_accuracy: 0.8750\n",
            "Epoch 101/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2718 - accuracy: 0.8698 - val_loss: 0.2823 - val_accuracy: 0.8750\n",
            "Epoch 102/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2717 - accuracy: 0.8698 - val_loss: 0.2821 - val_accuracy: 0.8750\n",
            "Epoch 103/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2716 - accuracy: 0.8698 - val_loss: 0.2819 - val_accuracy: 0.8750\n",
            "Epoch 104/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2714 - accuracy: 0.8698 - val_loss: 0.2817 - val_accuracy: 0.8750\n",
            "Epoch 105/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2712 - accuracy: 0.8698 - val_loss: 0.2816 - val_accuracy: 0.8750\n",
            "Epoch 106/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2710 - accuracy: 0.8698 - val_loss: 0.2815 - val_accuracy: 0.8750\n",
            "Epoch 107/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2709 - accuracy: 0.8698 - val_loss: 0.2814 - val_accuracy: 0.8750\n",
            "Epoch 108/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2707 - accuracy: 0.8698 - val_loss: 0.2813 - val_accuracy: 0.8750\n",
            "Epoch 109/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2706 - accuracy: 0.8698 - val_loss: 0.2811 - val_accuracy: 0.8750\n",
            "Epoch 110/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2706 - accuracy: 0.8698 - val_loss: 0.2810 - val_accuracy: 0.8750\n",
            "Epoch 111/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2704 - accuracy: 0.8698 - val_loss: 0.2808 - val_accuracy: 0.8750\n",
            "Epoch 112/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2703 - accuracy: 0.8750 - val_loss: 0.2806 - val_accuracy: 0.8750\n",
            "Epoch 113/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2701 - accuracy: 0.8750 - val_loss: 0.2805 - val_accuracy: 0.8750\n",
            "Epoch 114/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2701 - accuracy: 0.8750 - val_loss: 0.2804 - val_accuracy: 0.8750\n",
            "Epoch 115/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2699 - accuracy: 0.8750 - val_loss: 0.2803 - val_accuracy: 0.8750\n",
            "Epoch 116/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2698 - accuracy: 0.8698 - val_loss: 0.2802 - val_accuracy: 0.8750\n",
            "Epoch 117/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2696 - accuracy: 0.8750 - val_loss: 0.2800 - val_accuracy: 0.8750\n",
            "Epoch 118/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2695 - accuracy: 0.8750 - val_loss: 0.2801 - val_accuracy: 0.8750\n",
            "Epoch 119/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2697 - accuracy: 0.8750 - val_loss: 0.2798 - val_accuracy: 0.8750\n",
            "Epoch 120/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2694 - accuracy: 0.8750 - val_loss: 0.2799 - val_accuracy: 0.8750\n",
            "Epoch 121/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2692 - accuracy: 0.8750 - val_loss: 0.2798 - val_accuracy: 0.8750\n",
            "Epoch 122/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2692 - accuracy: 0.8750 - val_loss: 0.2796 - val_accuracy: 0.8750\n",
            "Epoch 123/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2690 - accuracy: 0.8750 - val_loss: 0.2796 - val_accuracy: 0.8958\n",
            "Epoch 124/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2691 - accuracy: 0.8750 - val_loss: 0.2794 - val_accuracy: 0.8958\n",
            "Epoch 125/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2689 - accuracy: 0.8750 - val_loss: 0.2794 - val_accuracy: 0.8958\n",
            "Epoch 126/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2689 - accuracy: 0.8750 - val_loss: 0.2792 - val_accuracy: 0.8958\n",
            "Epoch 127/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2687 - accuracy: 0.8750 - val_loss: 0.2793 - val_accuracy: 0.8958\n",
            "Epoch 128/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2686 - accuracy: 0.8750 - val_loss: 0.2791 - val_accuracy: 0.8958\n",
            "Epoch 129/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2685 - accuracy: 0.8750 - val_loss: 0.2790 - val_accuracy: 0.8958\n",
            "Epoch 130/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2684 - accuracy: 0.8750 - val_loss: 0.2790 - val_accuracy: 0.8958\n",
            "Epoch 131/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2684 - accuracy: 0.8750 - val_loss: 0.2789 - val_accuracy: 0.8958\n",
            "Epoch 132/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2682 - accuracy: 0.8750 - val_loss: 0.2788 - val_accuracy: 0.8958\n",
            "Epoch 133/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2681 - accuracy: 0.8750 - val_loss: 0.2787 - val_accuracy: 0.8958\n",
            "Epoch 134/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2681 - accuracy: 0.8750 - val_loss: 0.2787 - val_accuracy: 0.8958\n",
            "Epoch 135/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2680 - accuracy: 0.8750 - val_loss: 0.2785 - val_accuracy: 0.8958\n",
            "Epoch 136/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2679 - accuracy: 0.8750 - val_loss: 0.2786 - val_accuracy: 0.8958\n",
            "Epoch 137/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2680 - accuracy: 0.8750 - val_loss: 0.2786 - val_accuracy: 0.8958\n",
            "Epoch 138/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2678 - accuracy: 0.8750 - val_loss: 0.2786 - val_accuracy: 0.8958\n",
            "Epoch 139/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2677 - accuracy: 0.8750 - val_loss: 0.2783 - val_accuracy: 0.8958\n",
            "Epoch 140/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2676 - accuracy: 0.8750 - val_loss: 0.2784 - val_accuracy: 0.8958\n",
            "Epoch 141/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2675 - accuracy: 0.8750 - val_loss: 0.2783 - val_accuracy: 0.8958\n",
            "Epoch 142/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2675 - accuracy: 0.8750 - val_loss: 0.2782 - val_accuracy: 0.8958\n",
            "Epoch 143/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2675 - accuracy: 0.8750 - val_loss: 0.2781 - val_accuracy: 0.8958\n",
            "Epoch 144/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2674 - accuracy: 0.8750 - val_loss: 0.2781 - val_accuracy: 0.8958\n",
            "Epoch 145/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2673 - accuracy: 0.8750 - val_loss: 0.2781 - val_accuracy: 0.8958\n",
            "Epoch 146/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2672 - accuracy: 0.8750 - val_loss: 0.2780 - val_accuracy: 0.8958\n",
            "Epoch 147/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2672 - accuracy: 0.8750 - val_loss: 0.2781 - val_accuracy: 0.8958\n",
            "Epoch 148/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2670 - accuracy: 0.8750 - val_loss: 0.2779 - val_accuracy: 0.8958\n",
            "Epoch 149/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2670 - accuracy: 0.8750 - val_loss: 0.2779 - val_accuracy: 0.8958\n",
            "Epoch 150/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2669 - accuracy: 0.8750 - val_loss: 0.2779 - val_accuracy: 0.8958\n",
            "Epoch 151/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2669 - accuracy: 0.8750 - val_loss: 0.2779 - val_accuracy: 0.8958\n",
            "Epoch 152/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2668 - accuracy: 0.8750 - val_loss: 0.2779 - val_accuracy: 0.8958\n",
            "Epoch 153/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2669 - accuracy: 0.8750 - val_loss: 0.2778 - val_accuracy: 0.8958\n",
            "Epoch 154/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2667 - accuracy: 0.8750 - val_loss: 0.2778 - val_accuracy: 0.8958\n",
            "Epoch 155/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2667 - accuracy: 0.8750 - val_loss: 0.2777 - val_accuracy: 0.8958\n",
            "Epoch 156/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2666 - accuracy: 0.8750 - val_loss: 0.2778 - val_accuracy: 0.8958\n",
            "Epoch 157/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2666 - accuracy: 0.8750 - val_loss: 0.2777 - val_accuracy: 0.8958\n",
            "Epoch 158/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2665 - accuracy: 0.8750 - val_loss: 0.2776 - val_accuracy: 0.8958\n",
            "Epoch 159/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2665 - accuracy: 0.8750 - val_loss: 0.2775 - val_accuracy: 0.8958\n",
            "Epoch 160/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2666 - accuracy: 0.8750 - val_loss: 0.2776 - val_accuracy: 0.8958\n",
            "Epoch 161/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2664 - accuracy: 0.8750 - val_loss: 0.2775 - val_accuracy: 0.8958\n",
            "Epoch 162/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2664 - accuracy: 0.8750 - val_loss: 0.2775 - val_accuracy: 0.8958\n",
            "Epoch 163/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2663 - accuracy: 0.8750 - val_loss: 0.2775 - val_accuracy: 0.8958\n",
            "Epoch 164/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2663 - accuracy: 0.8750 - val_loss: 0.2775 - val_accuracy: 0.8958\n",
            "Epoch 165/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2662 - accuracy: 0.8750 - val_loss: 0.2775 - val_accuracy: 0.8958\n",
            "Epoch 166/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2662 - accuracy: 0.8750 - val_loss: 0.2773 - val_accuracy: 0.8958\n",
            "Epoch 167/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2661 - accuracy: 0.8750 - val_loss: 0.2773 - val_accuracy: 0.8958\n",
            "Epoch 168/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2663 - accuracy: 0.8750 - val_loss: 0.2775 - val_accuracy: 0.8958\n",
            "Epoch 169/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2661 - accuracy: 0.8750 - val_loss: 0.2774 - val_accuracy: 0.8958\n",
            "Epoch 170/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2660 - accuracy: 0.8750 - val_loss: 0.2772 - val_accuracy: 0.8958\n",
            "Epoch 171/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2660 - accuracy: 0.8750 - val_loss: 0.2771 - val_accuracy: 0.8958\n",
            "Epoch 172/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2661 - accuracy: 0.8750 - val_loss: 0.2772 - val_accuracy: 0.8958\n",
            "Epoch 173/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2659 - accuracy: 0.8750 - val_loss: 0.2772 - val_accuracy: 0.8958\n",
            "Epoch 174/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2659 - accuracy: 0.8750 - val_loss: 0.2772 - val_accuracy: 0.8958\n",
            "Epoch 175/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2658 - accuracy: 0.8750 - val_loss: 0.2772 - val_accuracy: 0.8958\n",
            "Epoch 176/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2658 - accuracy: 0.8750 - val_loss: 0.2771 - val_accuracy: 0.8958\n",
            "Epoch 177/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2657 - accuracy: 0.8750 - val_loss: 0.2770 - val_accuracy: 0.8958\n",
            "Epoch 178/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2658 - accuracy: 0.8750 - val_loss: 0.2770 - val_accuracy: 0.9167\n",
            "Epoch 179/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2657 - accuracy: 0.8750 - val_loss: 0.2772 - val_accuracy: 0.8958\n",
            "Epoch 180/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2656 - accuracy: 0.8750 - val_loss: 0.2771 - val_accuracy: 0.8958\n",
            "Epoch 181/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2656 - accuracy: 0.8750 - val_loss: 0.2772 - val_accuracy: 0.8958\n",
            "Epoch 182/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2656 - accuracy: 0.8750 - val_loss: 0.2771 - val_accuracy: 0.8958\n",
            "Epoch 183/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2655 - accuracy: 0.8750 - val_loss: 0.2770 - val_accuracy: 0.8958\n",
            "Epoch 184/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2655 - accuracy: 0.8750 - val_loss: 0.2771 - val_accuracy: 0.9167\n",
            "Epoch 185/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2655 - accuracy: 0.8750 - val_loss: 0.2770 - val_accuracy: 0.8958\n",
            "Epoch 186/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2656 - accuracy: 0.8750 - val_loss: 0.2770 - val_accuracy: 0.8958\n",
            "Epoch 187/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2654 - accuracy: 0.8750 - val_loss: 0.2770 - val_accuracy: 0.9167\n",
            "Epoch 188/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2654 - accuracy: 0.8750 - val_loss: 0.2769 - val_accuracy: 0.9167\n",
            "Epoch 189/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2655 - accuracy: 0.8750 - val_loss: 0.2768 - val_accuracy: 0.9167\n",
            "Epoch 190/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2654 - accuracy: 0.8750 - val_loss: 0.2770 - val_accuracy: 0.8958\n",
            "Epoch 191/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2654 - accuracy: 0.8750 - val_loss: 0.2769 - val_accuracy: 0.8958\n",
            "Epoch 192/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2654 - accuracy: 0.8750 - val_loss: 0.2769 - val_accuracy: 0.9167\n",
            "Epoch 193/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2653 - accuracy: 0.8750 - val_loss: 0.2769 - val_accuracy: 0.9167\n",
            "Epoch 194/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2653 - accuracy: 0.8750 - val_loss: 0.2770 - val_accuracy: 0.8958\n",
            "Epoch 195/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2652 - accuracy: 0.8750 - val_loss: 0.2770 - val_accuracy: 0.8958\n",
            "Epoch 196/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2651 - accuracy: 0.8750 - val_loss: 0.2769 - val_accuracy: 0.9167\n",
            "Epoch 197/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2651 - accuracy: 0.8750 - val_loss: 0.2768 - val_accuracy: 0.9167\n",
            "Epoch 198/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2652 - accuracy: 0.8750 - val_loss: 0.2769 - val_accuracy: 0.9167\n",
            "Epoch 199/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2652 - accuracy: 0.8750 - val_loss: 0.2769 - val_accuracy: 0.9167\n",
            "Epoch 200/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2651 - accuracy: 0.8750 - val_loss: 0.2768 - val_accuracy: 0.9167\n",
            "Epoch 201/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2651 - accuracy: 0.8750 - val_loss: 0.2769 - val_accuracy: 0.9167\n",
            "Epoch 202/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2651 - accuracy: 0.8750 - val_loss: 0.2769 - val_accuracy: 0.9167\n",
            "Epoch 203/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2650 - accuracy: 0.8750 - val_loss: 0.2768 - val_accuracy: 0.9167\n",
            "Epoch 204/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2650 - accuracy: 0.8750 - val_loss: 0.2768 - val_accuracy: 0.9167\n",
            "Epoch 205/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2650 - accuracy: 0.8750 - val_loss: 0.2768 - val_accuracy: 0.9167\n",
            "Epoch 206/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2652 - accuracy: 0.8750 - val_loss: 0.2769 - val_accuracy: 0.8958\n",
            "Epoch 207/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2650 - accuracy: 0.8750 - val_loss: 0.2768 - val_accuracy: 0.9167\n",
            "Epoch 208/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2649 - accuracy: 0.8750 - val_loss: 0.2768 - val_accuracy: 0.9167\n",
            "Epoch 209/500\n",
            "96/96 [==============================] - 0s 3ms/step - loss: 0.2651 - accuracy: 0.8750 - val_loss: 0.2769 - val_accuracy: 0.9167\n",
            "Epoch 210/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2649 - accuracy: 0.8750 - val_loss: 0.2769 - val_accuracy: 0.9167\n",
            "Epoch 211/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2651 - accuracy: 0.8750 - val_loss: 0.2768 - val_accuracy: 0.9167\n",
            "Epoch 212/500\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.2649 - accuracy: 0.8750 - val_loss: 0.2769 - val_accuracy: 0.9167\n",
            "Epoch 213/500\n",
            "75/96 [======================>.......] - ETA: 0s - loss: 0.2402 - accuracy: 0.8933"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgmAIMmBU2D5"
      },
      "source": [
        "* **(k)** Calculer les prédictions faites par le modèle et les stocker dans **`test_pred`**.\n",
        "> Nous disposons de  y_test et de test_pred. Cependant avant de pouvoir évaluer notre modèle, il va falloir à partir des predcitions, qui sont des probabilité d'appartenance à une classe, retrouver la classe prédite.\n",
        "\n",
        "* **(l)** Grâce à la méthode **where** du module **numpy**. A **test_pred**, associer les classes et les prédictions faites pour chaque observation de l'échantillon test.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-trBJ_NoU2D5"
      },
      "source": [
        "## Insérez votre code ici"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "DZ1iCo-MU2D5"
      },
      "source": [
        "test_pred = model.predict(X_test)\n",
        "classes_pred = np.where(test_pred>=0.5, 1,0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roiGDITlU2D5"
      },
      "source": [
        "* **(m)** Afficher le rapport de classification du modèle ainsi que la matrice de confusion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_246vTRxU2D6"
      },
      "source": [
        "## Insérez votre code ici"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "7xnAJHnBU2D6"
      },
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "print(classification_report(y_test, classes_pred))\n",
        "print(confusion_matrix(y_test,classes_pred))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFw6M8VTU2D6"
      },
      "source": [
        ">Le score pour ce modèle semble plutôt correct mais est-il plus ou moins performant qu'une régréssion logistique classique ? \n",
        ">\n",
        ">Pour vérifier cela, nous allons implémenter un modèle de régréssion logistique sur nos données.\n",
        ">\n",
        "* Exécuter la cellule suivante :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "wCAsvVLvU2D6"
      },
      "source": [
        "from sklearn import linear_model\n",
        "clf = linear_model.LogisticRegression(C = 1.0)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_rl = clf.predict(X_test) # Prédictions à l'aide de la régression logistique\n",
        "\n",
        "cm = pd.crosstab(y_test, y_pred_rl, rownames=['Classe réelle'], colnames=['Classe prédite'])\n",
        "cm # Matrice de confusion\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "run_control": {
          "frozen": true
        },
        "id": "1K4Z0zWjU2D6"
      },
      "source": [
        ">Les résultats sont donc identiques, ce qui confirme bien les suppositions de départ. \n",
        ">Seulement, l'algorithme de Perceptron simple n'est plus utilisé en pratique.\n",
        ">\n",
        "> L'interêt de l'algorithme du Perceptron vient d'une technique démontrée en 1989 par George Cybenko qui consiste à empiler plusieurs perceptrons qui auront la même entrée sur une couche appelée **couche cachée** (*hidden layer* en anglais). \n",
        ">\n",
        "> La sortie de cette couche de perceptrons sera ensuite donnée en entrée à un perceptron qui fera la classification binaire. Ce perceptron forme ce que l'on appelle la **couche de sortie** (*output layer* en anglais).\n",
        ">\n",
        "> Un algorithme de ce type s'appelle **Perceptron Multicouche** (*Multilayer Perceptron* en anglais), souvent abrégé par l'acronyme **MLP**.\n",
        ">\n",
        "> Dans l'exemple suivant, nous illustrons un MLP avec une couche cachée de 3 perceptrons et une couche de sortie à 1 perceptron. Vous pouvez appuyer sur le bouton *play* pour lancer l'animation, et le bouton *stop* pour la relancer depuis le début.\n",
        "\n",
        "* Exécuter la cellule suivante pour afficher l'interaction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "L2TonjgfU2D7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "0f5e751d-62dc-4d02-c4f4-3934c64b5841"
      },
      "source": [
        "from interaction_dense import show_dense\n",
        "show_dense()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b7775aa246d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0minteraction_dense\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_dense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mshow_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'interaction_dense'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fTxeelNU2D7"
      },
      "source": [
        ">Nous nous apercevons rapidement que la base de données utilisée précedemment n'est pas linéairement séparable. Pour ce type de données, aucun algorithme de Perceptron simple n'arrivera à trouver une solution satisfaisante.\n",
        ">\n",
        ">Néanmoins, grâce au modèle MLP nous pouvons approximer une frontière de décision non-linéaire. Dans la figure interactive ci- dessous, les points bleus et rouges représentent les deux classes d'individus et la ligne de couleur verte correspond à l'approximation de la frontière de décision obtenue par descente de gradient avec un maximum de $1 000 000$ d'itérations.\n",
        "\n",
        "* **(n)** À l'aide du menu intéractif, déterminer le meilleur pas de gradient pour un MLP ayant 3 perceptrons dans sa couche cachée et utilisant la fonction $ReLU$ comme activation.\n",
        "\n",
        "\n",
        "* **(o)** Pourquoi le MLP ayant 3 perceptrons dans sa couche cachée et utilisant la fonction $tanh$ comme activation ne trouve pas de frontière de décision satisfaisante pour un pas de gradient inférieur ou égal à $0.01$?\n",
        "\n",
        "\n",
        "* **(p)** Pour quelle raison le MLP ayant 30 perceptrons dans sa couche cachée et utilisant la fonction $ReLU$ comme activation est plus performant que le MLP ayant 90 perceptrons dans sa couche cachée et utilisant la fonction $ReLU$ comme activation lorsque le pas de gradient est fixé à $1$?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "_k8tCEAiU2D7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "bc8948d7-9897-4a63-e8bb-f238748a076f"
      },
      "source": [
        "from interaction_dense import show_mlp\n",
        "show_mlp()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-ecbefa415c81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0minteraction_dense\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_mlp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mshow_mlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'interaction_dense'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "run_control": {
          "frozen": true
        },
        "id": "nXvz7n9MU2D7"
      },
      "source": [
        ">Ainsi, si la base de données n'est pas linéairement séparable, **il est quand même possible de trouver un hyperplan séparateur non-linéaire en utilisant une approche multicouche.**\n"
      ]
    }
  ]
}